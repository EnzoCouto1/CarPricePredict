import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error

# 1. PREPARA√á√ÉO (Igualzinho antes)
df = pd.read_csv('car data.csv')
df['Car_Age'] = 2024 - df['Year']
df.drop(['Year', 'Car_Name'], axis=1, inplace=True)
df = pd.get_dummies(df, drop_first=True)

X = df.drop('Selling_Price', axis=1)
y = df['Selling_Price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- MODELO 1: Regress√£o Linear (O Simples/Baseline) ---
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
pred_lin = lin_reg.predict(X_test)
r2_lin = r2_score(y_test, pred_lin)
mae_lin = mean_absolute_error(y_test, pred_lin)

# --- MODELO 2: Random Forest (O Complexo/Desafiante) ---
# Random Forest cria centenas de "√°rvores de decis√£o" e tira a m√©dia.
# Geralmente √© muito bom para capturar padr√µes n√£o-lineares.
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
pred_rf = rf_reg.predict(X_test)
r2_rf = r2_score(y_test, pred_rf)
mae_rf = mean_absolute_error(y_test, pred_rf)

# --- RESULTADO FINAL: A Batalha ---
print("="*40)
print("     RELAT√ìRIO DE COMPARA√á√ÉO")
print("="*40)
print(f"{'M√©trica':<15} | {'Regress√£o Linear':<15} | {'Random Forest':<15}")
print("-" * 55)
print(f"{'R¬≤ Score':<15} | {r2_lin:.4f}          | {r2_rf:.4f}")
print(f"{'Erro MAE':<15} | {mae_lin:.4f}          | {mae_rf:.4f}")
print("-" * 55)

if r2_rf > r2_lin:
    print("\nüèÜ VENCEDOR: Random Forest (Capturou melhor os padr√µes complexos)")
else:
    print("\nüèÜ VENCEDOR: Regress√£o Linear (O problema √© simples e linear)")